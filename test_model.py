"""
Test GGUF Model
Kiá»ƒm tra model hoáº¡t Ä‘á»™ng tá»‘t trÆ°á»›c khi deploy
"""

import os
from pathlib import Path

GGUF_PATH = "models/cookshare.gguf"

print("=" * 60)
print("ğŸ§ª TEST GGUF MODEL")
print("=" * 60)

# Check file exists
if not os.path.exists(GGUF_PATH):
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y: {GGUF_PATH}")
    print("ğŸ‘‰ Cáº§n convert sang GGUF trÆ°á»›c (xem POST_TRAINING.md)")
    exit(1)

size_mb = os.path.getsize(GGUF_PATH) / (1024 * 1024)
print(f"âœ… File: {GGUF_PATH}")
print(f"ğŸ’¾ Size: {size_mb:.1f} MB")

# Try to load
try:
    from llama_cpp import Llama
    
    print("\nğŸ“¥ Loading model...")
    llm = Llama(
        model_path=GGUF_PATH,
        n_ctx=2048,
        n_batch=512,
        verbose=False
    )
    print("âœ… Model loaded successfully!")
    
    # Test prompt
    prompt = "<|im_start|>system\nBáº¡n lÃ  CookBot - AI tÆ° váº¥n mÃ³n Äƒn cá»§a CookShare. Tráº£ lá»i thÃ¢n thiá»‡n báº±ng tiáº¿ng Viá»‡t.<|im_end|>\n<|im_start|>user\nXin chÃ o!<|im_end|>\n<|im_start|>assistant\n"
    
    print("\nğŸ§ª Testing vá»›i prompt:")
    print(f"   {prompt[:100]}...")
    
    output = llm(
        prompt,
        max_tokens=100,
        temperature=0.7,
        top_p=0.9,
        stop=["<|im_end|>", "<|im_start|>"],
        echo=False
    )
    
    response = output["choices"][0]["text"]
    print(f"\nğŸ¤– Response: {response}")
    
    print("\n" + "=" * 60)
    print("âœ… MODEL HOáº T Äá»˜NG Tá»T!")
    print("=" * 60)
    print("\nğŸ“ Next steps:")
    print("   1. Deploy lÃªn Railway (xem POST_TRAINING.md)")
    print("   2. Hoáº·c test API local: python api.py")
    
except ImportError:
    print("\nâŒ llama-cpp-python chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    print("ğŸ‘‰ Cháº¡y: pip install llama-cpp-python")
    exit(1)
except Exception as e:
    print(f"\nâŒ Lá»—i khi test: {e}")
    exit(1)

