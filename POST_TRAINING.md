# ğŸ“¥ Sau Khi Download Model - HÆ°á»›ng Dáº«n Tiáº¿p Theo

**Báº¡n Ä‘Ã£ cÃ³ file `cookbot-merged.zip` - Giá» lÃ m gÃ¬?**

---

## ğŸ“‹ Tá»•ng Quan CÃ¡c BÆ°á»›c

1. âœ… **Giáº£i nÃ©n model** (2 phÃºt)
2. âœ… **Convert sang GGUF** (10-15 phÃºt)
3. âœ… **Test model** (5 phÃºt)
4. âœ… **Deploy lÃªn Railway** (10 phÃºt)

**Tá»•ng thá»i gian: ~30 phÃºt**

---

## ğŸ“¦ BÆ°á»›c 1: Giáº£i NÃ©n Model

### TrÃªn Windows:

1. **Click chuá»™t pháº£i** vÃ o `cookbot-merged.zip`
2. Chá»n **"Extract All..."** hoáº·c **"Extract to cookbot-merged\"**
3. Giáº£i nÃ©n vÃ o thÆ° má»¥c `models/` trong project:

```
F:\modelllmchatbot\
â””â”€â”€ models\
    â””â”€â”€ cookbot-merged\
        â”œâ”€â”€ config.json
        â”œâ”€â”€ model.safetensors
        â”œâ”€â”€ tokenizer.json
        â””â”€â”€ ... (cÃ¡c file khÃ¡c)
```

### Kiá»ƒm tra:

```bash
# Trong PowerShell
cd F:\modelllmchatbot
dir models\cookbot-merged
```

**Pháº£i tháº¥y file `model.safetensors` (~988MB)**

---

## ğŸ”„ BÆ°á»›c 2: Convert Sang GGUF Format

### CÃ¡ch 1: DÃ¹ng llama.cpp (Khuyáº¿n nghá»‹)

#### 2.1: Clone llama.cpp

```bash
# Má»Ÿ PowerShell hoáº·c CMD
cd F:\
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
```

#### 2.2: CÃ i Dependencies

```bash
# CÃ i Python packages
pip install -r requirements.txt

# Hoáº·c cÃ i thá»§ cÃ´ng
pip install numpy
```

#### 2.3: Convert Sang GGUF

```bash
# Convert tá»« HuggingFace format sang GGUF (f16)
python convert_hf_to_gguf.py F:\modelllmchatbot\models\cookbot-merged --outfile F:\modelllmchatbot\models\cookshare-f16.gguf --outtype f16
```

**Thá»i gian: ~5-10 phÃºt**

#### 2.4: Quantize (Giáº£m Size)

```bash
# Build llama-quantize (náº¿u chÆ°a cÃ³)
# Windows: Download pre-built tá»« releases
# Hoáº·c build tá»« source (phá»©c táº¡p hÆ¡n)

# Quantize sang Q4_K_M (balance giá»¯a size vÃ  quality)
# Náº¿u cÃ³ llama-quantize.exe:
.\llama-quantize.exe F:\modelllmchatbot\models\cookshare-f16.gguf F:\modelllmchatbot\models\cookshare.gguf q4_k_m
```

**Káº¿t quáº£:** File `cookshare.gguf` (~300-400MB)

---

### CÃ¡ch 2: DÃ¹ng Python Script (ÄÆ¡n giáº£n hÆ¡n)

Táº¡o file `convert_to_gguf.py`:

```python
"""
Convert HuggingFace model sang GGUF
Cáº§n cÃ i: pip install llama-cpp-python[server]
"""

import os
from pathlib import Path

# Paths
HF_MODEL_PATH = "models/cookbot-merged"
GGUF_OUTPUT = "models/cookshare.gguf"

print("ğŸ”„ Converting to GGUF...")
print(f"   Input: {HF_MODEL_PATH}")
print(f"   Output: {GGUF_OUTPUT}")

# Method 1: DÃ¹ng llama.cpp CLI (khuyáº¿n nghá»‹)
print("\nğŸ“ HÆ°á»›ng dáº«n:")
print("=" * 60)
print("""
1. Clone llama.cpp:
   git clone https://github.com/ggerganov/llama.cpp
   cd llama.cpp

2. Convert:
   python convert_hf_to_gguf.py ../models/cookbot-merged --outfile ../models/cookshare.gguf --outtype f16

3. Quantize (optional):
   ./llama-quantize cookshare.gguf cookshare-q4.gguf q4_k_m
""")
print("=" * 60)

# Method 2: Upload lÃªn Hugging Face vÃ  dÃ¹ng convert tool online
print("\nğŸ’¡ Hoáº·c upload lÃªn Hugging Face vÃ  dÃ¹ng convert tool online")
```

**Cháº¡y:**

```bash
python convert_to_gguf.py
```

---

## ğŸ§ª BÆ°á»›c 3: Test Model

**âš ï¸ LÆ°u Ã½:** TrÃªn Windows, `llama-cpp-python` cáº§n C compiler Ä‘á»ƒ build. KhÃ´ng thá»ƒ test local.

**Giáº£i phÃ¡p:** Test trÃªn Railway sau khi deploy (Railway sáº½ build tá»± Ä‘á»™ng).

### Hoáº·c: Test trÃªn Colab (Náº¿u muá»‘n test trÆ°á»›c)

1. Upload `cookshare.gguf` lÃªn Google Drive
2. Má»Ÿ Colab, mount Drive
3. Cháº¡y test script

### Test Script (Cháº¡y trÃªn Railway hoáº·c Colab)

File `test_model.py` Ä‘Ã£ cÃ³ sáºµn:

```python
"""
Test GGUF model
"""

from llama_cpp import Llama
import os

GGUF_PATH = "models/cookshare.gguf"

if not os.path.exists(GGUF_PATH):
    print(f"âŒ KhÃ´ng tÃ¬m tháº¥y: {GGUF_PATH}")
    print("ğŸ‘‰ Cáº§n convert sang GGUF trÆ°á»›c (xem POST_TRAINING.md)")
    exit(1)

print("ğŸ“¥ Loading model...")
llm = Llama(
    model_path=GGUF_PATH,
    n_ctx=2048,
    n_batch=512,
    verbose=False
)

print("âœ… Model loaded!")

# Test
prompt = "<|im_start|>system\nBáº¡n lÃ  CookBot - AI tÆ° váº¥n mÃ³n Äƒn cá»§a CookShare. Tráº£ lá»i thÃ¢n thiá»‡n báº±ng tiáº¿ng Viá»‡t.<|im_end|>\n<|im_start|>user\nXin chÃ o!<|im_end|>\n<|im_start|>assistant\n"

print("\nğŸ§ª Testing...")
print(f"Prompt: {prompt[:100]}...")

output = llm(
    prompt,
    max_tokens=100,
    temperature=0.7,
    top_p=0.9,
    stop=["<|im_end|>", "<|im_start|>"],
    echo=False
)

response = output["choices"][0]["text"]
print(f"\nğŸ¤– Response: {response}")

print("\nâœ… Model hoáº¡t Ä‘á»™ng tá»‘t!")
```

**Cháº¡y trÃªn Railway hoáº·c Colab:**

```bash
pip install llama-cpp-python
python test_model.py
```

**Hoáº·c:** Skip test, deploy Railway luÃ´n vÃ  test qua API endpoint `/health` vÃ  `/chat`

---

## ğŸš€ BÆ°á»›c 4: Deploy LÃªn Railway

### 4.1: Chuáº©n Bá»‹ Files

Äáº£m báº£o cÃ³ cÃ¡c file sau:

```
F:\modelllmchatbot\
â”œâ”€â”€ api.py
â”œâ”€â”€ model.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ models/
    â””â”€â”€ cookshare.gguf  â† File nÃ y!
```

### 4.2: Push LÃªn GitHub

```bash
# Khá»Ÿi táº¡o git (náº¿u chÆ°a cÃ³)
git init
git add .
git commit -m "Add trained model"

# Táº¡o repo trÃªn GitHub, rá»“i:
git remote add origin https://github.com/yourusername/cookshare-chatbot.git
git push -u origin main
```

### 4.3: Deploy Railway

1. **Truy cáº­p:** https://railway.app
2. **Login** báº±ng GitHub
3. **New Project** â†’ **Deploy from GitHub repo**
4. **Chá»n repo** `cookshare-chatbot`
5. **Set Environment Variables:**
   ```
   GGUF_MODEL_PATH=models/cookshare.gguf
   PORT=7860
   ```
6. **Deploy!**

### 4.4: Test API

Sau khi deploy xong, Railway sáº½ cung cáº¥p URL:

```bash
# Test health
curl https://your-app.railway.app/health

# Test chat
curl -X POST https://your-app.railway.app/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Xin chÃ o!"}'
```

---

## ğŸ“ Checklist

- [ ] Giáº£i nÃ©n `cookbot-merged.zip` vÃ o `models/cookbot-merged/`
- [ ] Convert sang GGUF â†’ `models/cookshare.gguf`
- [ ] Test model local â†’ `python test_model.py`
- [ ] Push code lÃªn GitHub
- [ ] Deploy lÃªn Railway
- [ ] Test API endpoint
- [ ] Káº¿t ná»‘i React Native app

---

## ğŸ†˜ Troubleshooting

### Lá»—i: "llama.cpp not found"
â†’ Clone llama.cpp vÃ  cÃ i dependencies

### Lá»—i: "convert_hf_to_gguf.py not found"
â†’ Äáº£m báº£o Ä‘ang á»Ÿ trong thÆ° má»¥c `llama.cpp/`

### Lá»—i: "Model too large for Railway"
â†’ Quantize xuá»‘ng Q4 hoáº·c Q5 (giáº£m size)

### Lá»—i: "API timeout"
â†’ Railway free tier cÃ³ giá»›i háº¡n, nÃªn upgrade hoáº·c dÃ¹ng VPS

---

## ğŸ¯ Next Steps

1. **Test model** â†’ Äáº£m báº£o hoáº¡t Ä‘á»™ng tá»‘t
2. **Deploy Railway** â†’ Public API
3. **Káº¿t ná»‘i React Native** â†’ Update API URL trong app
4. **Test end-to-end** â†’ Thá»­ chatbot trong app

---

**ğŸ‰ ChÃºc má»«ng! Model Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ deploy!**

