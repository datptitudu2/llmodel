"""
CookShare Chatbot Model
Wrapper cho llama-cpp-python v·ªõi model ƒë√£ train (.gguf)
CH·ªà d√πng model ƒë√£ train - KH√îNG c√≥ fallback
"""

import os
from typing import List, Tuple, Optional

class CookShareChatbot:
    """
    CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn cho ·ª©ng d·ª•ng CookShare
    
    Ki·∫øn tr√∫c:
    - Model: Fine-tuned Qwen2-0.5B ‚Üí cookshare.gguf
    - Engine: llama-cpp-python
    - CH·ªà d√πng model ƒë√£ train - KH√îNG c√≥ fallback
    
    Features:
    - G·ª£i √Ω c√¥ng th·ª©c t·ª´ nguy√™n li·ªáu
    - H∆∞·ªõng d·∫´n n·∫•u t·ª´ng b∆∞·ªõc
    - Thay th·∫ø nguy√™n li·ªáu
    - ƒêi·ªÅu ch·ªânh kh·∫©u ph·∫ßn
    - C·∫£nh b√°o d·ªã ·ª©ng/ki√™ng k·ªµ
    - L√™n l·ªãch ƒÉn, g·ª£i √Ω theo th·ªùi ti·∫øt
    - ∆Ø·ªõc t√≠nh chi ph√≠
    """
    
    def __init__(self):
        """
        Initialize chatbot
        CH·ªà d√πng model ƒë√£ train (cookshare.gguf)
        KH√îNG c√≥ fallback v·ªÅ model ch∆∞a train
        """
        # Config paths
        self.gguf_model_path = os.getenv("GGUF_MODEL_PATH", "models/cookshare.gguf")
        
        # Engine state
        self.llm = None  # llama-cpp model
        
        # System prompt (context c∆° b·∫£n - model ƒë√£ h·ªçc t·ª´ training data)
        self.system_prompt = "B·∫°n l√† CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn c·ªßa CookShare. Tr·∫£ l·ªùi th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát."
        
        # Initialize
        self._initialize()
    
    def _initialize(self):
        """Initialize model engine - CH·ªà d√πng model ƒë√£ train"""
        # Ki·ªÉm tra file GGUF c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(self.gguf_model_path):
            warning_msg = f"‚ö†Ô∏è  CH∆ØA T√åM TH·∫§Y MODEL: {self.gguf_model_path}\n" \
                         f"üëâ Service s·∫Ω start nh∆∞ng ch∆∞a th·ªÉ tr·∫£ l·ªùi.\n" \
                         f"üëâ Model s·∫Ω ƒë∆∞·ª£c download t·ª´ Google Drive trong Dockerfile."
            print(warning_msg)
            self.llm = None  # Model ch∆∞a load
            return
        
        print(f"üîç T√¨m th·∫•y model ƒë√£ train: {self.gguf_model_path}")
        self._load_gguf_model()
    
    def _load_gguf_model(self):
        """Load GGUF model v·ªõi llama-cpp-python - Model ƒë√£ train l√† b·∫Øt bu·ªôc"""
        try:
            from llama_cpp import Llama
            
            print(f"üì• ƒêang load model ƒë√£ train...")
            
            # Detect GPU (tr√™n Railway th∆∞·ªùng kh√¥ng c√≥ GPU)
            n_gpu_layers = 0
            try:
                import torch
                if torch.cuda.is_available():
                    n_gpu_layers = -1  # Use all GPU layers
                    print(f"üéÆ GPU detected: {torch.cuda.get_device_name(0)}")
            except ImportError:
                pass
            
            # Load model ƒë√£ train
            # T·ªëi ∆∞u cho t·ªëc ƒë·ªô: gi·∫£m n_ctx, t·∫Øt verbose, tƒÉng threads, d√πng mmap
            self.llm = Llama(
                model_path=self.gguf_model_path,
                n_ctx=512,            # Gi·∫£m context window xu·ªëng 512 ƒë·ªÉ tƒÉng t·ªëc ƒë√°ng k·ªÉ
                n_batch=128,          # Gi·∫£m batch size xu·ªëng 128 ƒë·ªÉ tƒÉng t·ªëc
                n_gpu_layers=n_gpu_layers,
                verbose=False,        # T·∫Øt verbose ƒë·ªÉ tƒÉng t·ªëc
                n_threads=8,          # TƒÉng threads (Railway c√≥ 8 vCPUs)
                use_mmap=True,        # D√πng memory mapping ƒë·ªÉ tƒÉng t·ªëc load
                use_mlock=False       # Kh√¥ng lock memory (ti·∫øt ki·ªám RAM)
            )
            
            print("‚úÖ Model ƒë√£ train loaded successfully!")
            
        except ImportError:
            error_msg = "‚ùå llama-cpp-python ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t.\n" \
                       "üëâ Model ƒë√£ train y√™u c·∫ßu llama-cpp-python. " \
                       "Vui l√≤ng c√†i ƒë·∫∑t trong Dockerfile."
            print(error_msg)
            raise RuntimeError(error_msg)
            
        except Exception as e:
            error_msg = f"‚ùå KH√îNG TH·ªÇ LOAD MODEL ƒê√É TRAIN: {e}\n" \
                       "üëâ File GGUF c√≥ th·ªÉ b·ªã corrupt ho·∫∑c convert kh√¥ng ƒë√∫ng.\n" \
                       "üëâ C·∫ßn re-upload file l√™n Google Drive ho·∫∑c convert l·∫°i."
            print(error_msg)
            # Kh√¥ng raise error, ƒë·ªÉ service v·∫´n start ƒë∆∞·ª£c
            # Model s·∫Ω ƒë∆∞·ª£c load l·∫°i khi c√≥ request (n·∫øu file ƒë∆∞·ª£c fix)
            self.llm = None
    
    def _format_prompt(self, messages: List[dict]) -> str:
        """
        Format messages sang ChatML format
        Compatible v·ªõi Qwen2 model
        """
        prompt = ""
        has_system = False
        
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            if role == "system":
                prompt += f"<|im_start|>system\n{content}<|im_end|>\n"
                has_system = True
            elif role == "user":
                prompt += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                prompt += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        
        # Th√™m system prompt n·∫øu ch∆∞a c√≥
        if not has_system:
            prompt = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n" + prompt
        
        # Trigger assistant response
        prompt += "<|im_start|>assistant\n"
        
        return prompt
    
    def _generate_gguf(self, messages: List[dict]) -> str:
        """
        Generate response t·ª´ GGUF model
        """
        if self.llm is None:
            # Th·ª≠ load l·∫°i model n·∫øu ch∆∞a load
            self._initialize()
            if self.llm is None:
                return "Xin l·ªói, model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ki·ªÉm tra logs."

        try:
            prompt = self._format_prompt(messages)
            
            output = self.llm(
                prompt,
                max_tokens=256,       # Gi·∫£m xu·ªëng 256 ƒë·ªÉ tƒÉng t·ªëc ƒë√°ng k·ªÉ (v·∫´n ƒë·ªß cho c√¢u tr·∫£ l·ªùi ng·∫Øn)
                temperature=0.5,      # Gi·∫£m temperature ƒë·ªÉ response ch√≠nh x√°c h∆°n, √≠t hallucination
                top_p=0.8,            # Gi·∫£m top_p ƒë·ªÉ t·∫≠p trung v√†o tokens c√≥ x√°c su·∫•t cao
                top_k=40,             # Gi·ªõi h·∫°n top_k ƒë·ªÉ tr√°nh ch·ªçn tokens l·∫°
                repeat_penalty=1.3,   # TƒÉng penalty ƒë·ªÉ tr√°nh l·∫∑p l·∫°i (1.0 = kh√¥ng penalty, >1.0 = penalty)
                stop=["<|im_end|>", "<|im_start|>", "\n\n\n"],  # Stop s·ªõm khi g·∫∑p stop token ho·∫∑c nhi·ªÅu newlines
                echo=False
            )
            
            response = output["choices"][0]["text"]
            return self._clean_response(response)
            
        except Exception as e:
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o ph·∫£n h·ªìi: {str(e)}"
    
    def _clean_response(self, text: str) -> str:
        """
        Clean up response text - Remove duplicates v√† format
        """
        import re
        
        # Remove trailing tags
        for tag in ["<|im_end|>", "<|im_start|>assistant", "<|im_start|>user", "<|im_start|>system"]:
            text = text.replace(tag, "")
        
        # Clean up multiple newlines (gi·ªØ l·∫°i \n\n nh∆∞ng lo·∫°i b·ªè \n\n\n\n...)
        text = re.sub(r'\n{3,}', '\n\n', text)  # Thay nhi·ªÅu \n b·∫±ng \n\n
        
        # Remove duplicate sentences/phrases
        # Split th√†nh sentences (d·ª±a v√†o d·∫•u ch·∫•m, ch·∫•m h·ªèi, ch·∫•m than, xu·ªëng d√≤ng)
        sentences = re.split(r'[.!?]\s+|\n+', text)
        seen = set()
        unique_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # Normalize sentence ƒë·ªÉ so s√°nh (lowercase, remove extra spaces)
            normalized = re.sub(r'\s+', ' ', sentence.lower().strip())
            
            # B·ªè qua c√¢u qu√° ng·∫Øn (c√≥ th·ªÉ l√† d·∫•u c√¢u)
            if len(normalized) < 5:
                unique_sentences.append(sentence)
                continue
            
            # Ki·ªÉm tra duplicate (cho ph√©p m·ªôt s·ªë kh√°c bi·ªát nh·ªè)
            is_duplicate = False
            for seen_sentence in seen:
                # N·∫øu c√¢u m·ªõi gi·ªëng >80% v·ªõi c√¢u ƒë√£ th·∫•y th√¨ coi l√† duplicate
                similarity = self._calculate_similarity(normalized, seen_sentence)
                if similarity > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                seen.add(normalized)
                unique_sentences.append(sentence)
        
        # Join l·∫°i th√†nh text
        text = '\n'.join(unique_sentences)
        
        # Remove duplicate lines (exact match)
        lines = text.split('\n')
        seen_lines = set()
        unique_lines = []
        for line in lines:
            line_stripped = line.strip()
            if line_stripped and line_stripped not in seen_lines:
                seen_lines.add(line_stripped)
                unique_lines.append(line)
        text = '\n'.join(unique_lines)
        
        # Strip whitespace
        text = text.strip()
        
        return text
    
    def _calculate_similarity(self, s1: str, s2: str) -> float:
        """
        T√≠nh similarity gi·ªØa 2 strings (simple word overlap)
        """
        words1 = set(s1.split())
        words2 = set(s2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        if not union:
            return 0.0
        
        return len(intersection) / len(union)
    
    def get_response(self, user_message: str, history: List[Tuple[str, str]] = None) -> str:
        """
        Get response t·ª´ chatbot
        
        Args:
            user_message: C√¢u h·ªèi c·ªßa user
            history: L·ªãch s·ª≠ chat [(user_msg, assistant_msg), ...]
        
        Returns:
            Response t·ª´ chatbot
        """
        if history is None:
            history = []
        
        # Build messages
        messages = []
        
        # Add system prompt
        messages.append({"role": "system", "content": self.system_prompt})
        
        # Add history
        for user_msg, assistant_msg in history:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": assistant_msg})
        
        # Add current message
        messages.append({"role": "user", "content": user_message})
        
        # Get response
        response = self._generate_gguf(messages)
        
        return response.strip()
    
    def get_model_info(self) -> dict:
        """Get info v·ªÅ model ƒëang d√πng"""
        return {
            "model_path": self.gguf_model_path,
            "model_loaded": self.llm is not None,
            "model_type": "Fine-tuned Qwen2-0.5B (cookshare.gguf)" if self.llm else "Not loaded",
        }


# Test
if __name__ == "__main__":
    print("ü§ñ Testing CookBot...")
    
    bot = CookShareChatbot()
    print(f"Model info: {bot.get_model_info()}")
    
    # Test questions
    test_questions = [
        "Xin ch√†o",
        "M√¨nh c√≥ tr·ª©ng v√† c√† chua, l√†m m√≥n g√¨?",
        "C√°ch l√†m ph·ªü b√≤?",
    ]
    
    for q in test_questions:
        print(f"\nüë§ User: {q}")
        response = bot.get_response(q)
        print(f"ü§ñ Bot: {response[:200]}..." if len(response) > 200 else f"ü§ñ Bot: {response}")
