"""
CookShare Chatbot Model
Sá»­ dá»¥ng Fine-tuned Model hoáº·c Hugging Face Inference API
"""

import os
from typing import List, Tuple, Optional
import requests

class CookShareChatbot:
    """
    CookBot - AI tÆ° váº¥n mÃ³n Äƒn cho á»©ng dá»¥ng CookShare
    
    Features:
    - Gá»£i Ã½ cÃ´ng thá»©c tá»« nguyÃªn liá»‡u
    - HÆ°á»›ng dáº«n náº¥u tá»«ng bÆ°á»›c
    - Thay tháº¿ nguyÃªn liá»‡u
    - Äiá»u chá»‰nh kháº©u pháº§n
    - Cáº£nh bÃ¡o dá»‹ á»©ng/kiÃªng ká»µ
    - LÃªn lá»‹ch Äƒn, gá»£i Ã½ theo thá»i tiáº¿t
    - Æ¯á»›c tÃ­nh chi phÃ­
    """
    
    def __init__(self):
        """
        Initialize chatbot
        Priority: Fine-tuned model > Inference API > Base model
        """
        # Config
        self.use_inference_api = os.getenv("USE_INFERENCE_API", "true").lower() == "true"
        self.api_token = os.getenv("HF_TOKEN", "")
        
        # Model paths
        self.finetuned_model_path = os.getenv("FINETUNED_MODEL_PATH", "models/cookbot-finetuned")
        self.base_model_name = "microsoft/Phi-3-mini-4k-instruct"
        
        # Current model (will be set later)
        self.model = None
        self.tokenizer = None
        self.model_loaded = False
        
        # System prompt (QUAN TRá»ŒNG: GiÃºp model tráº£ lá»i chÃ­nh xÃ¡c, Ä‘Ãºng format nhÆ° training data)
        self.system_prompt = """Báº¡n lÃ  CookBot - AI tÆ° váº¥n mÃ³n Äƒn cá»§a CookShare. 

QUY Táº®C QUAN TRá»ŒNG:
1. CHá»ˆ Ä‘Æ°a ra thÃ´ng tin CHÃNH XÃC vá» nguyÃªn liá»‡u, cÃ´ng thá»©c náº¥u Äƒn
2. KHÃ”NG Ä‘Æ°á»£c bá»‹a Ä‘áº·t nguyÃªn liá»‡u khÃ´ng tá»“n táº¡i (nhÆ° xÃ  phÃ²ng, bá»™t nÆ°á»›c, nÆ°á»›c thay nÆ°á»›c, etc.)
3. CHá»ˆ dÃ¹ng nguyÃªn liá»‡u thá»±c pháº©m THáº¬T: thá»‹t, rau, gia vá»‹, nÆ°á»›c máº¯m, Ä‘Æ°á»ng, muá»‘i, dáº§u Äƒn, etc.
4. Tráº£ lá»i theo FORMAT trong training data:
   - DÃ¹ng emoji phÃ¹ há»£p (ðŸš ðŸœ ðŸ¥¢ ðŸ³)
   - CÃ³ thÃ´ng tin: â± Thá»i gian, ðŸ“Š Äá»™ khÃ³, ðŸ‘¥ Kháº©u pháº§n
   - Liá»‡t kÃª nguyÃªn liá»‡u rÃµ rÃ ng
   - HÆ°á»›ng dáº«n tá»«ng bÆ°á»›c chi tiáº¿t
   - CÃ³ máº¹o náº¥u Äƒn
5. Tráº£ lá»i NGáº®N Gá»ŒN, RÃ• RÃ€NG, Dá»„ HIá»‚U
6. Náº¿u khÃ´ng cháº¯c cháº¯n, hÃ£y nÃ³i "TÃ´i chÆ°a cÃ³ thÃ´ng tin chÃ­nh xÃ¡c vá» mÃ³n nÃ y"
7. LuÃ´n nháº¯c nhá»Ÿ vá» an toÃ n thá»±c pháº©m khi cáº§n

Tráº£ lá»i thÃ¢n thiá»‡n báº±ng tiáº¿ng Viá»‡t."""
        
        # Initialize
        self._initialize()
    
    def _initialize(self):
        """Initialize model hoáº·c API"""
        # Thá»­ load fine-tuned model trÆ°á»›c
        if os.path.exists(self.finetuned_model_path) and not self.use_inference_api:
            print(f"ðŸ” TÃ¬m tháº¥y fine-tuned model: {self.finetuned_model_path}")
            self._load_finetuned_model()
        elif self.use_inference_api and self.api_token:
            print("âœ… Sá»­ dá»¥ng Hugging Face Inference API")
        else:
            print("âš ï¸  KhÃ´ng cÃ³ fine-tuned model, sáº½ dÃ¹ng Inference API")
            self.use_inference_api = True
    
    def _load_finetuned_model(self):
        """Load fine-tuned model (LoRA)"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from peft import PeftModel
            import torch
            
            print(f"ðŸ“¥ Äang load fine-tuned model...")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.finetuned_model_path)
            
            # Load base model
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            
            # Load LoRA adapter
            self.model = PeftModel.from_pretrained(base_model, self.finetuned_model_path)
            self.model.eval()
            
            self.model_loaded = True
            self.use_inference_api = False
            print("âœ… Fine-tuned model loaded successfully!")
            
        except Exception as e:
            print(f"âŒ Lá»—i load fine-tuned model: {e}")
            print("Falling back to Inference API...")
            self.use_inference_api = True
            self.model_loaded = False
    
    def _load_base_model(self):
        """Load base model (fallback)"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            print(f"ðŸ“¥ Äang load base model: {self.base_model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            
            self.model_loaded = True
            print("âœ… Base model loaded successfully!")
            
        except Exception as e:
            print(f"âŒ Lá»—i load base model: {e}")
            self.use_inference_api = True
    
    def _format_messages(self, messages: List[dict]) -> str:
        """
        Format messages theo Phi-3 chat template
        """
        formatted = ""
        has_system = False
        
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            if role == "system":
                formatted += f"<|system|>\n{content}<|end|>\n"
                has_system = True
            elif role == "user":
                formatted += f"<|user|>\n{content}<|end|>\n"
            elif role == "assistant":
                formatted += f"<|assistant|>\n{content}<|end|>\n"
        
        # ThÃªm system prompt náº¿u chÆ°a cÃ³
        if not has_system:
            formatted = f"<|system|>\n{self.system_prompt}<|end|>\n" + formatted
        
        # ThÃªm assistant tag Ä‘á»ƒ model tiáº¿p tá»¥c
        formatted += "<|assistant|>\n"
        
        return formatted
    
    def _call_inference_api(self, messages: List[dict]) -> str:
        """
        Gá»i Hugging Face Inference API
        """
        # Thá»­ fine-tuned model trÃªn HF Hub trÆ°á»›c, rá»“i base model
        model_to_use = os.getenv("HF_MODEL_ID", self.base_model_name)
        api_url = f"https://api-inference.huggingface.co/models/{model_to_use}"
        
        headers = {}
        if self.api_token:
            headers["Authorization"] = f"Bearer {self.api_token}"
        
         payload = {
             "inputs": self._format_messages(messages),
             "parameters": {
                 "max_new_tokens": 512,      # Giáº£m Ä‘á»ƒ tÄƒng tá»‘c vÃ  trÃ¡nh hallucination
                 "temperature": 0.5,         # Giáº£m Ä‘á»ƒ response chÃ­nh xÃ¡c hÆ¡n
                 "top_p": 0.8,               # Giáº£m Ä‘á»ƒ táº­p trung vÃ o tokens cÃ³ xÃ¡c suáº¥t cao
                 "top_k": 40,                # Giá»›i háº¡n top_k Ä‘á»ƒ trÃ¡nh chá»n tokens láº¡
                 "do_sample": True,
                 "return_full_text": False,
                 "repetition_penalty": 1.2   # TrÃ¡nh láº·p láº¡i
             }
         }
        
        try:
            response = requests.post(api_url, json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            result = response.json()
            
            if isinstance(result, list) and len(result) > 0:
                text = result[0].get("generated_text", "")
            elif isinstance(result, dict):
                text = result.get("generated_text", "")
            else:
                text = "Xin lá»—i, mÃ¬nh khÃ´ng thá»ƒ tráº£ lá»i cÃ¢u há»i nÃ y."
            
            # Clean up response
            text = self._clean_response(text)
            return text
            
        except requests.exceptions.RequestException as e:
            return f"Xin lá»—i, cÃ³ lá»—i káº¿t ná»‘i: {str(e)}"
    
    def _generate_local(self, messages: List[dict]) -> str:
        """
        Generate response tá»« local model
        """
        try:
            import torch
            
            formatted_input = self._format_messages(messages)
            inputs = self.tokenizer(formatted_input, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
             with torch.no_grad():
                 outputs = self.model.generate(
                     **inputs,
                     max_new_tokens=512,      # Giáº£m Ä‘á»ƒ tÄƒng tá»‘c vÃ  trÃ¡nh hallucination
                     temperature=0.5,         # Giáº£m Ä‘á»ƒ response chÃ­nh xÃ¡c hÆ¡n
                     top_p=0.8,               # Giáº£m Ä‘á»ƒ táº­p trung vÃ o tokens cÃ³ xÃ¡c suáº¥t cao
                     top_k=40,                # Giá»›i háº¡n top_k Ä‘á»ƒ trÃ¡nh chá»n tokens láº¡
                     do_sample=True,
                     pad_token_id=self.tokenizer.eos_token_id,
                     repetition_penalty=1.2   # TrÃ¡nh láº·p láº¡i
                 )
            
            # Decode only new tokens
            response = self.tokenizer.decode(
                outputs[0][inputs["input_ids"].shape[1]:], 
                skip_special_tokens=True
            )
            
            # Clean up
            response = self._clean_response(response)
            return response
            
        except Exception as e:
            return f"Xin lá»—i, cÃ³ lá»—i xáº£y ra: {str(e)}"
    
    def _clean_response(self, text: str) -> str:
        """
        Clean up response text
        """
        # Remove trailing tags
        for tag in ["<|end|>", "<|assistant|>", "<|user|>", "<|system|>"]:
            text = text.replace(tag, "")
        
        # Strip whitespace
        text = text.strip()
        
        return text
    
    def get_response(self, user_message: str, history: List[Tuple[str, str]] = None) -> str:
        """
        Get response tá»« chatbot
        
        Args:
            user_message: CÃ¢u há»i cá»§a user
            history: Lá»‹ch sá»­ chat [(user_msg, assistant_msg), ...]
        
        Returns:
            Response tá»« chatbot
        """
        if history is None:
            history = []
        
        # Build messages
        messages = []
        
        # Add system prompt
        messages.append({"role": "system", "content": self.system_prompt})
        
        # Add history
        for user_msg, assistant_msg in history:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": assistant_msg})
        
        # Add current message
        messages.append({"role": "user", "content": user_message})
        
        # Get response
        if self.use_inference_api or not self.model_loaded:
            response = self._call_inference_api(messages)
        else:
            response = self._generate_local(messages)
        
        return response.strip()
    
    def get_model_info(self) -> dict:
        """Get info vá» model Ä‘ang dÃ¹ng"""
        return {
            "using_inference_api": self.use_inference_api,
            "model_loaded": self.model_loaded,
            "finetuned_path": self.finetuned_model_path if os.path.exists(self.finetuned_model_path) else None,
            "base_model": self.base_model_name,
        }


# Test
if __name__ == "__main__":
    print("ðŸ¤– Testing CookBot...")
    
    bot = CookShareChatbot()
    print(f"Model info: {bot.get_model_info()}")
    
    # Test questions
    test_questions = [
        "Xin chÃ o",
        "MÃ¬nh cÃ³ trá»©ng vÃ  cÃ  chua, lÃ m mÃ³n gÃ¬?",
        "CÃ¡ch lÃ m phá»Ÿ bÃ²?",
    ]
    
    for q in test_questions:
        print(f"\nðŸ‘¤ User: {q}")
        response = bot.get_response(q)
        print(f"ðŸ¤– Bot: {response[:200]}..." if len(response) > 200 else f"ðŸ¤– Bot: {response}")
