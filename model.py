"""
CookShare Chatbot Model
Wrapper cho llama-cpp-python v·ªõi model ƒë√£ train (.gguf)
CH·ªà d√πng model ƒë√£ train - KH√îNG c√≥ fallback
"""

import os
from typing import List, Tuple, Optional

class CookShareChatbot:
    """
    CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn cho ·ª©ng d·ª•ng CookShare
    
    Ki·∫øn tr√∫c:
    - Model: Fine-tuned Qwen2-0.5B ‚Üí cookshare.gguf
    - Engine: llama-cpp-python
    - CH·ªà d√πng model ƒë√£ train - KH√îNG c√≥ fallback
    
    Features:
    - G·ª£i √Ω c√¥ng th·ª©c t·ª´ nguy√™n li·ªáu
    - H∆∞·ªõng d·∫´n n·∫•u t·ª´ng b∆∞·ªõc
    - Thay th·∫ø nguy√™n li·ªáu
    - ƒêi·ªÅu ch·ªânh kh·∫©u ph·∫ßn
    - C·∫£nh b√°o d·ªã ·ª©ng/ki√™ng k·ªµ
    - L√™n l·ªãch ƒÉn, g·ª£i √Ω theo th·ªùi ti·∫øt
    - ∆Ø·ªõc t√≠nh chi ph√≠
    """
    
    def __init__(self):
        """
        Initialize chatbot
        CH·ªà d√πng model ƒë√£ train (cookshare.gguf)
        KH√îNG c√≥ fallback v·ªÅ model ch∆∞a train
        """
        # Config paths
        self.gguf_model_path = os.getenv("GGUF_MODEL_PATH", "models/cookshare.gguf")
        
        # Engine state
        self.llm = None  # llama-cpp model
        
        # System prompt (context c∆° b·∫£n - model ƒë√£ h·ªçc t·ª´ training data)
        self.system_prompt = "B·∫°n l√† CookBot - AI t∆∞ v·∫•n m√≥n ƒÉn c·ªßa CookShare. Tr·∫£ l·ªùi th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát."
        
        # Initialize
        self._initialize()
    
    def _initialize(self):
        """Initialize model engine - CH·ªà d√πng model ƒë√£ train"""
        # Ki·ªÉm tra file GGUF c√≥ t·ªìn t·∫°i kh√¥ng
        if not os.path.exists(self.gguf_model_path):
            warning_msg = f"‚ö†Ô∏è  CH∆ØA T√åM TH·∫§Y MODEL: {self.gguf_model_path}\n" \
                         f"üëâ Service s·∫Ω start nh∆∞ng ch∆∞a th·ªÉ tr·∫£ l·ªùi.\n" \
                         f"üëâ Upload file model qua Railway CLI: railway upload models/cookshare.gguf\n" \
                         f"üëâ Sau ƒë√≥ restart service."
            print(warning_msg)
            self.llm = None  # Model ch∆∞a load
            return
        
        print(f"üîç T√¨m th·∫•y model ƒë√£ train: {self.gguf_model_path}")
        self._load_gguf_model()
    
    def _load_gguf_model(self):
        """Load GGUF model v·ªõi llama-cpp-python - Model ƒë√£ train l√† b·∫Øt bu·ªôc"""
        try:
            from llama_cpp import Llama
            
            print(f"üì• ƒêang load model ƒë√£ train...")
            
            # Detect GPU (tr√™n Railway th∆∞·ªùng kh√¥ng c√≥ GPU)
            n_gpu_layers = 0
            try:
                import torch
                if torch.cuda.is_available():
                    n_gpu_layers = -1  # Use all GPU layers
                    print(f"üéÆ GPU detected: {torch.cuda.get_device_name(0)}")
            except ImportError:
                pass
            
            # Load model ƒë√£ train
            # T·ªëi ∆∞u cho t·ªëc ƒë·ªô: gi·∫£m n_ctx, t·∫Øt verbose, tƒÉng threads
            self.llm = Llama(
                model_path=self.gguf_model_path,
                n_ctx=1024,           # Gi·∫£m context window ƒë·ªÉ tƒÉng t·ªëc
                n_batch=256,          # Gi·∫£m batch size ƒë·ªÉ tƒÉng t·ªëc
                n_gpu_layers=n_gpu_layers,
                verbose=False,        # T·∫Øt verbose ƒë·ªÉ tƒÉng t·ªëc
                n_threads=8           # TƒÉng threads (Railway c√≥ 8 vCPUs)
            )
            
            print("‚úÖ Model ƒë√£ train loaded successfully!")
            
        except ImportError:
            error_msg = "‚ùå llama-cpp-python ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t.\n" \
                       "üëâ Model ƒë√£ train y√™u c·∫ßu llama-cpp-python. " \
                       "Vui l√≤ng c√†i ƒë·∫∑t trong Dockerfile."
            print(error_msg)
            raise RuntimeError(error_msg)
            
        except Exception as e:
            error_msg = f"‚ùå KH√îNG TH·ªÇ LOAD MODEL ƒê√É TRAIN: {e}\n" \
                       "üëâ File GGUF c√≥ th·ªÉ b·ªã corrupt ho·∫∑c convert kh√¥ng ƒë√∫ng.\n" \
                       "üëâ C·∫ßn re-upload file l√™n Google Drive ho·∫∑c convert l·∫°i."
            print(error_msg)
            # Kh√¥ng raise error, ƒë·ªÉ service v·∫´n start ƒë∆∞·ª£c
            # Model s·∫Ω ƒë∆∞·ª£c load l·∫°i khi c√≥ request (n·∫øu file ƒë∆∞·ª£c fix)
            self.llm = None
    
    def _format_prompt(self, messages: List[dict]) -> str:
        """
        Format messages sang ChatML format
        Compatible v·ªõi nhi·ªÅu model (Qwen, Phi, Llama, etc.)
        """
        prompt = ""
        has_system = False
        
        for msg in messages:
            role = msg["role"]
            content = msg["content"]
            
            if role == "system":
                prompt += f"<|im_start|>system\n{content}<|im_end|>\n"
                has_system = True
            elif role == "user":
                prompt += f"<|im_start|>user\n{content}<|im_end|>\n"
            elif role == "assistant":
                prompt += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        
        # Th√™m system prompt n·∫øu ch∆∞a c√≥
        if not has_system:
            prompt = f"<|im_start|>system\n{self.system_prompt}<|im_end|>\n" + prompt
        
        # Trigger assistant response
        prompt += "<|im_start|>assistant\n"
        
        return prompt
    
    def _generate_gguf(self, messages: List[dict]) -> str:
        """
        Generate response t·ª´ GGUF model
        """
        try:
            prompt = self._format_prompt(messages)
            
            output = self.llm(
                prompt,
                max_tokens=1024,
                temperature=0.7,
                top_p=0.9,
                stop=["<|im_end|>", "<|im_start|>"],
                echo=False
            )
            
            response = output["choices"][0]["text"]
            return self._clean_response(response)
            
        except Exception as e:
            return f"Xin l·ªói, c√≥ l·ªói x·∫£y ra: {str(e)}"
    
    
    def _clean_response(self, text: str) -> str:
        """Clean up response text"""
        # Remove common tags
        tags_to_remove = [
            "<|im_end|>", "<|im_start|>", "<|end|>", 
            "<|assistant|>", "<|user|>", "<|system|>"
        ]
        for tag in tags_to_remove:
            text = text.replace(tag, "")
        
        return text.strip()
    
    def get_response(self, user_message: str, history: List[Tuple[str, str]] = None) -> str:
        """
        Get response t·ª´ chatbot
        
        Args:
            user_message: C√¢u h·ªèi c·ªßa user
            history: L·ªãch s·ª≠ chat [(user_msg, assistant_msg), ...]
        
        Returns:
            Response t·ª´ chatbot
        """
        # Ki·ªÉm tra model ƒë√£ load ch∆∞a
        if self.llm is None:
            if not os.path.exists(self.gguf_model_path):
                return "‚ö†Ô∏è Model ch∆∞a ƒë∆∞·ª£c upload. Vui l√≤ng upload file cookshare.gguf qua Railway CLI v√† restart service."
            # Th·ª≠ load l·∫°i model (c√≥ th·ªÉ ƒë√£ upload sau khi start)
            print("üîÑ Th·ª≠ load model l·∫°i...")
            self._load_gguf_model()
            if self.llm is None:
                return "‚ö†Ô∏è Kh√¥ng th·ªÉ load model. Vui l√≤ng ki·ªÉm tra logs."
        
        if history is None:
            history = []
        
        # Build messages
        messages = [{"role": "system", "content": self.system_prompt}]
        
        # Add history
        for user_msg, assistant_msg in history:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": assistant_msg})
        
        # Add current message
        messages.append({"role": "user", "content": user_message})
        
        # Generate response t·ª´ model ƒë√£ train
        response = self._generate_gguf(messages)
        
        return response.strip()
    
    def get_model_info(self) -> dict:
        """Get info v·ªÅ model ƒëang d√πng"""
        return {
            "engine": "llama-cpp-python",
            "model_path": self.gguf_model_path,
            "model_loaded": self.llm is not None,
            "model_type": "Fine-tuned Qwen2-0.5B (cookshare.gguf)",
        }


# Test
if __name__ == "__main__":
    print("=" * 50)
    print("ü§ñ Testing CookBot")
    print("=" * 50)
    
    bot = CookShareChatbot()
    print(f"\nüìä Model info: {bot.get_model_info()}")
    
    # Test questions
    test_questions = [
        "Xin ch√†o!",
        "M√¨nh c√≥ tr·ª©ng v√† c√† chua, l√†m m√≥n g√¨?",
        "H∆∞·ªõng d·∫´n c√°ch l√†m ph·ªü b√≤",
    ]
    
    for q in test_questions:
        print(f"\nüë§ User: {q}")
        response = bot.get_response(q)
        print(f"ü§ñ Bot: {response[:300]}..." if len(response) > 300 else f"ü§ñ Bot: {response}")
